{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np # for transformation\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from os.path import exists\n",
    "\n",
    "import torch # PyTorch package\n",
    "import torchvision # load datasets\n",
    "import torchvision.transforms as transforms # transform data\n",
    "import torch.nn as nn # basic building block for neural neteorks\n",
    "import torch.nn.functional as F # import convolution functions like Relu\n",
    "import torch.optim as optim # optimzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data into a python dictionary\n",
    "train_data = pd.read_json(\"train_data.json\")\n",
    "\n",
    "#print(train_data)\n",
    "\n",
    "# Clean out the games that have no reviews\n",
    "train_df = train_data.dropna(subset=[\"sentiment\"])\n",
    "\n",
    "# explode the dataset\n",
    "train_df_expanded=train_df.explode(\"screenshots\", ignore_index=True)\n",
    "#train_df_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['appid', 'release', 'title', 'price', 'sentiment', 'reviews',\n",
      "       'percentage', 'tags', 'screenshots'],\n",
      "      dtype='object')\n",
      "Positive                   2053\n",
      "Very Positive              1469\n",
      "Mixed                       922\n",
      "Mostly Positive             811\n",
      "Overwhelmingly Positive     130\n",
      "Mostly Negative             107\n",
      "Negative                     11\n",
      "Very Negative                 2\n",
      "Name: sentiment, dtype: int64\n",
      "No value has a count of 2.\n"
     ]
    }
   ],
   "source": [
    "#Exploration and minor cleaning\n",
    "print(train_data.columns)\n",
    "\n",
    "print(train_data[\"sentiment\"].value_counts())\n",
    "\n",
    "dupe = train_data[\"appid\"].value_counts()\n",
    "if 2 in dupe.values:\n",
    "    print(\"There is at least one value with a count of 2.\")\n",
    "else:\n",
    "    print(\"No value has a count of 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_expanded[\"images\"] = None\n",
    "for i in range(len(train_df_expanded[\"screenshots\"])):\n",
    "    screenshot_filename = train_df_expanded[\"screenshots\"][i]\n",
    "    image_path = f\"C:/Users/nicol/OneDrive - KU Leuven/Desktop/python/Advanced Analytics in Business/2/images/{screenshot_filename}\"\n",
    "    if exists(image_path):\n",
    "        with Image.open(image_path) as image:\n",
    "            train_df_expanded.at[i, \"images\"] = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean out the games that have no image\n",
    "train_df_expanded = train_df_expanded.dropna(subset=[\"images\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 24883200 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exists(image_path):\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(image_path) \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[1;32m---> 10\u001b[0m             transformed_image \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m             train_df_expanded\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m transformed_image\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Clean out the games that have no image\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 24883200 bytes."
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose( # composing several transforms together\n",
    "    [transforms.ToTensor(), # to tensor object\n",
    "     transforms.Normalize((127.5, 127.5, 127.5), (127.5, 127.5, 127.5))])\n",
    "\n",
    "train_df_expanded[\"images\"] = None\n",
    "for i in range(len(train_df_expanded[\"screenshots\"])):\n",
    "    screenshot_filename = train_df_expanded[\"screenshots\"][i]\n",
    "    if exists(image_path):\n",
    "        with Image.open(image_path) as image:\n",
    "            transformed_image = transform(image)\n",
    "            train_df_expanded.at[i, \"images\"] = transformed_image\n",
    "\n",
    "# Clean out the games that have no image\n",
    "train_df_expanded = train_df_expanded.dropna(subset=[\"images\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set batch_size\n",
    "batch_size = 4\n",
    "\n",
    "# set number of workers\n",
    "num_workers = 2\n",
    "\n",
    "# train data\n",
    "trainloader = torch.utils.data.DataLoader(train_df_expanded, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# put 10 classes into a set\n",
    "classes = ('Overwhelmingly Positive', 'Very Positive', 'Positive', 'Mostly Positive',\n",
    "           'Mixed', 'Mostly Negative', 'Negative', 'Very Negative', 'Ovewhelmingly Negative')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
